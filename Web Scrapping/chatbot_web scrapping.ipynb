{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_cases(s):\n",
    "    s.loc[\"Active cases\"]=str(int(s.loc[\"Coronavirus Cases\"].replace(',',''))-int(s.loc[\"Deaths\"].replace(',',''))-int(s.loc[\"Recovered\"].replace(',','')))\n",
    "    count=0\n",
    "    number=''\n",
    "    for digit in s.loc[\"Active cases\"][::-1]:\n",
    "        if count==3:\n",
    "            number+=','\n",
    "            count=0\n",
    "        number+=digit\n",
    "        count+=1\n",
    "    s.loc[\"Active cases\"]=number[::-1]\n",
    "    return s\n",
    "\n",
    "def dump_obj(path,obj):\n",
    "    f=open(path,'wb')\n",
    "    pickle.dump(obj,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real time data : Summary of cases , World-wide news ,WHO news\n",
    "\n",
    "website='https://pandemic.internationalsos.com/2019-ncov/covid-19-daily-summary'\n",
    "source=requests.get(website).text\n",
    "soup=BeautifulSoup(source,'html.parser')\n",
    "covid_summary=soup.find('div',{'class':'module freetext large var1 bodycontent'}).find('div',{'class':'wrapper cf'})\n",
    "\n",
    "for index in range(0,len(covid_summary.find_all('p'))-5):\n",
    "    try:\n",
    "        if index==0:\n",
    "            published_time=covid_summary.find_all('p')[index].text\n",
    "        elif index==1:\n",
    "            details=covid_summary.find_all('p')[index].text\n",
    "        elif index==4:\n",
    "            who_news=covid_summary.find_all('p')[index].text+covid_summary.find_all('p')[index+1].text\n",
    "            countrylist=['WHO Summary']\n",
    "            newslist=[who_news]\n",
    "        elif index>=6:\n",
    "            country,news=covid_summary.find_all('p')[index].text.split(':')\n",
    "            countrylist.append(country)\n",
    "            newslist.append(news)\n",
    "    except:\n",
    "        pass\n",
    "newsdf=pd.DataFrame({'Country':countrylist,'News':newslist})\n",
    "\n",
    "index=['Last updated']\n",
    "data=[published_time.split('\\n')[0]]\n",
    "for i in (details.split('\\n')):\n",
    "    a,b=i.split(':')\n",
    "    index.append(a)\n",
    "    data.append(b.strip())\n",
    "s=pd.Series(data,index)\n",
    "\n",
    "dump_obj('covid_summary.pkl',s)\n",
    "\n",
    "dump_obj('countrywise_news.pkl',newsdf)\n",
    "\n",
    "#print(s)\n",
    "#print(newsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mythbusters\n",
    "\n",
    "who_homepage=\"https://www.who.int/emergencies/diseases/novel-coronavirus-2019\"\n",
    "source1=requests.get(who_homepage+'/advice-for-public/myth-busters').text\n",
    "soup1=BeautifulSoup(source1,'lxml')\n",
    "mythbusters=soup1.find_all('div',{'class':'sf-content-block content-block'})\n",
    "d={'Headlines':[],'Content':[]}\n",
    "for mythbuster in mythbusters:\n",
    "    try:\n",
    "        headline=mythbuster.div.h2.text\n",
    "        content=mythbuster.div.text\n",
    "        d['Headlines'].append(headline)\n",
    "        d['Content'].append(content)\n",
    "    except:\n",
    "        pass\n",
    "mythbuster_df=pd.DataFrame(d)\n",
    "\n",
    "#dump_obj('D:\\\\Jupyter notebooks\\\\Cods summer project 2020_Chatbot\\\\Static Data\\\\mythbusters.pkl',mythbuster_df)\n",
    "\n",
    "#print(mythbuster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Covid-19 detailed realtime updates country_wise\n",
    "\n",
    "link='https://www.worldometers.info/coronavirus/'\n",
    "source2=requests.get(link).text\n",
    "soup2=BeautifulSoup(source2,'lxml')\n",
    "rows=soup2.find('table',{'id':'main_table_countries_today'})\n",
    "column_names=[\"Country1\"]\n",
    "ths=rows.find_all('th')\n",
    "for th in ths:\n",
    "    if th.text.strip()!='#':\n",
    "        column_names.append(th.text.replace('\\n',''))\n",
    "rows=rows.tbody.find_all(\"tr\")\n",
    "d=dict()\n",
    "country_links={}\n",
    "for index in range(len(rows)):\n",
    "    try:\n",
    "        countryinfo=rows[index].find_all('a',href=True)[0]\n",
    "        country=countryinfo.text\n",
    "        country_link=countryinfo['href']\n",
    "        country_links[country]=link+country_link\n",
    "        data=[i.text for i in rows[index].find_all('td')]+[link+country_link]\n",
    "    except:\n",
    "        country=rows[index].find_all('td')[0].text\n",
    "        data=[i.text for i in rows[index].find_all('td')]+[np.NaN]\n",
    "    d[country]=data\n",
    "\n",
    "column_names=column_names+[\"Link for more about the country\"]\n",
    "realtime_data=pd.DataFrame(d).iloc[1:,:].T.reset_index()\n",
    "realtime_data.columns=column_names\n",
    "del realtime_data['Country1']\n",
    "\n",
    "\n",
    "dump_obj('worldwide_statistics.pkl',realtime_data)\n",
    "    \n",
    "#print(realtime_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#USA statewise\n",
    "link=list(country_links.values())[0]\n",
    "source=requests.get(link).text\n",
    "soup=BeautifulSoup(source,'lxml')\n",
    "rows=soup.find('table',{'id':'usa_table_countries_today'})\n",
    "rows1=rows.find_all('tbody')[0].find_all(\"tr\")\n",
    "rows2=rows.find_all('tbody')[1].find_all(\"tr\")\n",
    "rows=rows1+rows2\n",
    "state_d={}\n",
    "state_links={}\n",
    "for index in range(len(rows)):\n",
    "    try:\n",
    "        stateinfo=rows[index].find_all('a',href=True)[0]\n",
    "        state_link=stateinfo['href']\n",
    "        if state_link.startswith('http'):\n",
    "            data=[i.text.strip() for i in row[index].find_all('td')]+[state_link]\n",
    "        else:\n",
    "            data=[i.text.strip() for i in rows[index].find_all('td')]+[link[:-1]+state_link]\n",
    "        state=data[0]\n",
    "        if state_link.startswith('http'):\n",
    "            state_links[state]=state_link\n",
    "        else:\n",
    "            state_links[state]=link[:-1]+state_link\n",
    "    except:\n",
    "        data=[i.text.strip() for i in rows[index].find_all('td')]+[np.NaN]\n",
    "        state=data[0]\n",
    "    state_d[state]=data\n",
    "     \n",
    "\n",
    "column_names=[\"State\",\"Total Cases\",\"New Cases\", \"Total Deaths\", \"New Deaths\",\"Active Cases\",\"Total Cases/1M population\",\"Deaths/1M population\",\"Total tests\",\"Total tests/1M population\",'a','b',\"Link for more about the state\"]\n",
    "usa_state_data=pd.DataFrame(state_d).iloc[1:,:].T.reset_index()\n",
    "usa_state_data.columns=column_names\n",
    "del usa_state_data['a']\n",
    "del usa_state_data['b']\n",
    "\n",
    "dump_obj('USA_state_statistics.pkl',usa_state_data)\n",
    "\n",
    "#print(usa_state_data)\n",
    "#print(state_links)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USA county-wise data\n",
    "\n",
    "for state_link in state_links:\n",
    "        link=state_links[state_link]\n",
    "        source=requests.get(link).text\n",
    "        soup=BeautifulSoup(source,'lxml')\n",
    "        try:\n",
    "            rows=soup.find('table',{'id':'usa_table_countries_today'})\n",
    "            rows=rows.find_all('tbody')[0].find_all(\"tr\")\n",
    "            county_d={}\n",
    "            for index in range(len(rows)):\n",
    "                    data=[i.text.strip() for i in rows[index].find_all('td')]\n",
    "                    county=data[0]\n",
    "                    county_d[county]=data\n",
    "\n",
    "            column_names=[\"County\",\"Total Cases\",\"New Cases\", \"Total Deaths\", \"New Deaths\",\"Active Cases\",\"Total tests\",'a']\n",
    "            usa_county_data=pd.DataFrame(county_d).iloc[1:,:].T.reset_index()\n",
    "            usa_county_data.columns=column_names\n",
    "            del usa_county_data['a']\n",
    "\n",
    "            dump_obj(\"D:\\\\Jupyter notebooks\\\\Cods summer project 2020_Chatbot\\\\USA_countywise_statistics\\\\\"+state_link+'_statistics.pkl',usa_county_data)\n",
    "        except:\n",
    "            county_d={}\n",
    "            rows=soup.find_all(\"div\",{'id':'maincounter-wrap'})\n",
    "            for row in rows[:-1]:\n",
    "                title=row.h1.text.strip()[:-1]\n",
    "                data=row.div.text.strip()\n",
    "                county_d[title]=data\n",
    "            county_s=pd.Series(county_d)\n",
    "            county_s=active_cases(county_s)\n",
    "                \n",
    "            dump_obj(\"D:\\\\Jupyter notebooks\\\\Cods summer project 2020_Chatbot\\\\USA_countywise_statistics\\\\\"+state_link+'_statistics.pkl',county_s)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Countries(except USA) data\n",
    "\n",
    "for country in country_links:\n",
    "    try:\n",
    "        if country!='USA':\n",
    "            country_d={}\n",
    "            link=country_links[country]\n",
    "            source=requests.get(link).text\n",
    "            soup=BeautifulSoup(source,'lxml')\n",
    "            rows=soup.find_all(\"div\",{'id':'maincounter-wrap'})[:3]\n",
    "            for row in rows:\n",
    "                title=row.h1.text.strip()[:-1]\n",
    "                data=row.div.text.strip()\n",
    "                country_d[title]=data\n",
    "            rows1=soup.find_all('div',{'class':'col-md-6'})\n",
    "            if len(rows1)==2:\n",
    "                data1=rows1[0].find('div',{'style':'float:left; text-align:center'}).span.text\n",
    "                title1=rows1[0].find('div',{'style':'float:left; text-align:center'}).div.text\n",
    "                data2=rows1[0].find('div',{'style':'float:right; text-align:center'}).span.text\n",
    "                title2=rows1[0].find('div',{'style':'float:right; text-align:center'}).div.text\n",
    "                country_d[title1]=data1\n",
    "                country_d[title2]=data2\n",
    "            country_s=pd.Series(country_d)\n",
    "            if country_s.loc['Recovered']!='N/A':\n",
    "                active_cases(country_s)\n",
    "            dump_obj(\"D:\\\\Jupyter notebooks\\\\Cods summer project 2020_Chatbot\\\\countrywise_statistics\\\\\"+country+'_statistics.pkl',country_s)\n",
    "    except:\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realtime covid-19 summary\n",
    "\n",
    "link='https://www.worldometers.info/coronavirus/'\n",
    "source=requests.get(link).text\n",
    "soup=BeautifulSoup(source,'lxml')\n",
    "divs=soup.find_all('div',{'id':'maincounter-wrap'})\n",
    "d={}\n",
    "for div in divs:\n",
    "    title=div.h1.text[:-1]\n",
    "    data=div.span.text\n",
    "    d[title]=data\n",
    "box=soup.find('div',{'class':'col-md-6'})\n",
    "data1=box.find('div',{'style':'float:left; text-align:center'}).span.text\n",
    "title1=box.find('div',{'style':'float:left; text-align:center'}).div.text\n",
    "data2=box.find('div',{'style':'float:right; text-align:center'}).span.text\n",
    "title2=box.find('div',{'style':'float:right; text-align:center'}).div.text\n",
    "d[title1]=data1\n",
    "d[title2]=data2\n",
    "s=pd.Series(d)\n",
    "active_cases(s)\n",
    "dump_obj('covid_summary_realtime.pkl',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covid 19 incubation period\n",
    "\n",
    "new_link='https://www.worldometers.info/coronavirus/coronavirus-incubation-period/'\n",
    "source=requests.get(new_link).text\n",
    "soup=BeautifulSoup(source,'lxml')\n",
    "d={'Heading':[],'Content':[]}\n",
    "d['Heading'].append(soup.find('h4',{'id':'incubation-estimates'}).text.strip())\n",
    "d['Heading'].append(soup.find('h3').text.strip())\n",
    "d['Heading'].append(soup.find('h4',{'id':'24'}).text.strip())\n",
    "for ul in soup.find_all('ul')[1:-2]:\n",
    "    d['Content'].append(ul.text.strip())\n",
    "d1={'Virus':[],'Incubation period':[]}\n",
    "td=soup.find_all('td')\n",
    "for i in td:\n",
    "    if i['align']=='left':\n",
    "        d1['Virus'].append(i.text.strip())\n",
    "    else:\n",
    "        d1['Incubation period'].append(i.text.replace('\\n',''))\n",
    "df1=pd.DataFrame(d1)\n",
    "d['Heading'].append('Comparission Table')\n",
    "d['Content'].append(df1)\n",
    "df=pd.DataFrame(d)\n",
    "\n",
    "dump_obj('D:\\\\Jupyter notebooks\\\\Cods summer project 2020_Chatbot\\\\Static Data\\\\incubation_period.pkl',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q/A about the virus by WHO\n",
    "\n",
    "qna_link='https://www.who.int/emergencies/diseases/novel-coronavirus-2019/question-and-answers-hub'\n",
    "source=requests.get(qna_link).text\n",
    "soup=BeautifulSoup(source,'lxml')\n",
    "q_links={}\n",
    "for q in zip(soup.find_all('span',{'class':'trimmed'}),soup.find_all('a',{'class':'sf-list-vertical__item'})):\n",
    "    q_links[q[0].text]='https://www.who.int'+q[1]['href']\n",
    "for q in q_links:\n",
    "        new_source=requests.get(q_links[q]).text\n",
    "        new_soup=BeautifulSoup(new_source,'lxml')\n",
    "        qna={'Questions':[],'Answers':[]}\n",
    "        question=new_soup.find('div',{'class':'qa-details__content'}).find('div',{'class':'col-md-7'}).find_all('a',{'class':'sf-accordion__link'})\n",
    "        answer=new_soup.find('div',{'class':'qa-details__content'}).find('div',{'class':'col-md-7'}).find_all('div',{'class':'sf-accordion__content'})\n",
    "        for que in zip(question,answer):\n",
    "            qna['Questions'].append(que[0].text.strip())\n",
    "            qna['Answers'].append(que[1].text.strip())\n",
    "        df=pd.DataFrame(qna)\n",
    "        \n",
    "        #dump_obj(\"D:\\\\Jupyter notebooks\\\\Cods summer project 2020_Chatbot\\\\Q and A\\\\\"+q.replace(':',' on').replace('/',' OR ')+'.pkl',df)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Preventive measures , Masks , sanitizers:\n",
    "\n",
    "link='http://who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public'\n",
    "source=requests.get(link).text\n",
    "soup=BeautifulSoup(source,'lxml')\n",
    "measures={}\n",
    "heading=soup.find_all('div',{'class':\"section-heading\"})[3:1:-1]\n",
    "m=soup.find('div',{'class':'sf_colsIn col-md-10'}).find_all('div',{'class':'sf-content-block content-block'})[2:4]\n",
    "for i in range(len(m)):\n",
    "    measures[heading[i].text.strip()]=m[i].text.strip().split('.')[:-1]\n",
    "\n",
    "link2='https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public/when-and-how-to-use-masks'\n",
    "source2=requests.get(link2).text\n",
    "soup2=BeautifulSoup(source2,'lxml')\n",
    "heading2=soup2.find('div',{'class':'sf_colsIn col-md-9'}).find_all('div',{'class':'section-heading'})[:2]\n",
    "m2=soup2.find('div',{'class':'sf_colsIn col-md-9'}).find_all('div',{'class':'sf-content-block content-block'})[:2]\n",
    "for i in range(len(m2)):\n",
    "    measures[heading2[i].text.strip()]=m2[i].text.replace('&amp;amp;amp;amp;nbsp;','').strip().split('.')[:-1]\n",
    "\n",
    "max_size=max([len(measures[x]) for x in measures])\n",
    "for i in measures:\n",
    "    if(len(measures[i])!=max_size):\n",
    "        measures[i]+=(['']*(max_size-len(measures[i])))\n",
    "\n",
    "df=pd.DataFrame(measures)\n",
    "\n",
    "#dump_obj('D:\\\\Jupyter notebooks\\\\Cods summer project 2020_Chatbot\\\\Static Data\\\\Protective_measures(masks,sanitizers etc).pkl',df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WHO, Gavi and UNICEF warn that disruption to routine vaccination leaves at least 80 million children at risk  \n"
     ]
    }
   ],
   "source": [
    "#rolling updates on COVID-19 by WHO:\n",
    "\n",
    "link='https://www.who.int/emergencies/diseases/novel-coronavirus-2019/events-as-they-happen'\n",
    "source=requests.get(link).text\n",
    "soup=BeautifulSoup(source,'lxml')\n",
    "news={'Date':[],'Headline':[],'News':[]}\n",
    "topics=soup.find_all('div',{'class':'sf-content-block content-block'})\n",
    "for topic in topics:\n",
    "    try:\n",
    "        date=topic.h2\n",
    "    except:\n",
    "        date='31 December 2019'\n",
    "    headline=topic.find('span',{'style':'background-color:transparent;font-size:25px;font-weight:700;text-align:inherit;white-space:inherit;word-spacing:normal;caret-color:auto;'})\n",
    "    try:\n",
    "        print(date.text,headline.text)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f=open('.pkl','rb')\n",
    "#print(pickle.load(f))\n",
    "#f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
